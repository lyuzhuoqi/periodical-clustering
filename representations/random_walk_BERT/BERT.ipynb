{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating PID to model input IDs...\n",
      "Finished. PID to model input IDs (first 5 items): {'<pad>': 0, '<mask>': 1, 11223: 2, 12345: 3, 44556: 4}\n",
      "Total number of unique PIDs (plus padding value): 7\n",
      "Print a batch:\n",
      "tensor([[2, 4, 3, 5, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 5, 2, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[4, 6, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Shape of embedding layer: torch.Size([7, 768])\n",
      "Epoch 1, Loss: 3.3882\n",
      "Epoch 2, Loss: 2.5744\n",
      "Epoch 3, Loss: 2.8493\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 示例数据：每条citation_trail可能长度不同\n",
    "citation_trail = [\n",
    "    [12345, 67890, 11223],        # 长度为3\n",
    "    [44556, 77889],               # 长度为2\n",
    "    [11223, 44556, 12345, 67890]  # 长度为4\n",
    "]\n",
    "\n",
    "# 设置特殊 token（如果需要）\n",
    "special_tokens = {\n",
    "    '<pad>': 0,\n",
    "    '<mask>': 1,  # 通常需要一个 mask token 用于掩码语言模型\n",
    "}\n",
    "\n",
    "# 获取所有唯一的PID\n",
    "all_pids = set()\n",
    "for trail in citation_trail:\n",
    "    all_pids.update(trail)  # 将每个引用链中的PID加入到set中\n",
    "\n",
    "# 创建 PID 到索引的映射，索引从 1 开始，保留 0 作为填充\n",
    "print(\"Start creating PID to model input IDs...\")\n",
    "pid_to_idx = {**special_tokens, **{pid: idx + len(special_tokens) for idx, pid in enumerate(sorted(all_pids))}}\n",
    "from itertools import islice\n",
    "num_items = 5\n",
    "first_items = dict(islice(pid_to_idx.items(), num_items))\n",
    "print(\"Finished. PID to model input IDs (first 5 items):\", first_items)\n",
    "num_pids = len(pid_to_idx)\n",
    "print(f\"Total number of unique PIDs (plus padding value): {num_pids}\")\n",
    "\n",
    "# 设置最大长度\n",
    "max_length = 10\n",
    "padding_value = 0\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 自定义数据集类\n",
    "class CitationTrailDataset(Dataset):\n",
    "    def __init__(self, citation_trails, pid_to_idx, max_length=5, pad_value=padding_value):\n",
    "        self.citation_trails = citation_trails\n",
    "        self.pid_to_idx = pid_to_idx\n",
    "        self.max_length = max_length\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.citation_trails)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trail = self.citation_trails[idx]\n",
    "        # 将 PID 转换为索引\n",
    "        trail = [self.pid_to_idx.get(pid, self.pad_value) for pid in trail]\n",
    "        if len(trail) > self.max_length:\n",
    "            trail = trail[:self.max_length]\n",
    "        trail = trail + [self.pad_value] * (self.max_length - len(trail))\n",
    "        \n",
    "        return torch.tensor(trail, dtype=torch.long)\n",
    "\n",
    "# 创建数据集并传入 PID 到索引的映射\n",
    "dataset = CitationTrailDataset(citation_trail, pid_to_idx, max_length=max_length)\n",
    "# 创建DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# 打印一个batch的数据\n",
    "print(\"Print a batch:\")\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "# 创建自定义的 RobertaConfig\n",
    "config = RobertaConfig(\n",
    "    vocab_size=num_pids,          # 设置词汇表大小\n",
    "    pad_token_id=pid_to_idx['<pad>'],  # 设置填充 token 的 id\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=514,  # 通常为512 + 2\n",
    ")\n",
    "# 使用自定义配置初始化模型\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "# 查看embedding层\n",
    "print(\"Shape of embedding layer:\", model.roberta.embeddings.word_embeddings.weight.shape)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# 使用GPU（如果可用）\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# 训练循环\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch.to(device)\n",
    "\n",
    "        # 创建注意力掩码（真实 token 为 1，填充为 0）\n",
    "        attention_mask = (inputs != padding_value).long()\n",
    "\n",
    "        # 准备标签用于掩码语言建模\n",
    "        labels = inputs.clone()\n",
    "        rand = torch.rand(inputs.shape).to(device)\n",
    "        mask_arr = (rand < 0.15) * (labels != padding_value)  # 15% 的掩盖\n",
    "\n",
    "        # 如果当前批次没有被掩盖的 token，手动掩盖一个 token\n",
    "        if mask_arr.sum() == 0:\n",
    "            # 获取所有非填充 token 的索引\n",
    "            non_pad_indices = (labels != padding_value).nonzero(as_tuple=False)\n",
    "            if non_pad_indices.numel() > 0:\n",
    "                # 随机选择一个非填充 token 的位置\n",
    "                idx = non_pad_indices[torch.randint(0, non_pad_indices.size(0), (1,)).item()]\n",
    "                mask_arr[idx[0], idx[1]] = True\n",
    "\n",
    "        # 更新 labels，只保留被掩盖 token 的原始值，其余设置为 -100\n",
    "        labels[~mask_arr] = -100\n",
    "\n",
    "        # 替换被掩盖的 token 为 <mask> token\n",
    "        inputs[mask_arr] = pid_to_idx['<mask>']\n",
    "\n",
    "        # 添加断言以确保输入有效\n",
    "        assert inputs.max() < num_pids, f\"Input ID {inputs.max()} 超出词汇表范围 (vocab_size={num_pids})\"\n",
    "        assert inputs.min() >= 0, f\"Input ID {inputs.min()} 为负数\"\n",
    "        assert (labels != -100).sum() > 0, \"当前批次没有被掩盖的 token，无法计算损失\"\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 可选：梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 12345, \n",
      "Embedding: [-1.51849678e-02 -1.69298649e-02  1.37473112e-02  1.45901022e-02\n",
      " -1.46607589e-02  5.85053535e-03 -2.43586022e-02  1.64092856e-03\n",
      "  2.37097917e-03 -1.72557272e-02 -1.76676475e-02 -1.88329685e-02\n",
      " -5.31993173e-02  2.04114011e-03  1.12546491e-03  1.91159244e-03\n",
      "  5.82950423e-03  1.28991753e-02 -1.06629888e-02  1.12393694e-02\n",
      " -5.55666350e-03 -1.47349834e-02  1.85030848e-02 -1.12575823e-02\n",
      "  8.79686978e-03 -1.96818006e-03 -2.48249825e-02  1.17827738e-02\n",
      " -9.48400889e-03 -5.76315587e-03  2.57337326e-03 -5.57135837e-03\n",
      "  1.88882370e-02 -1.47756711e-02  6.67076977e-03 -1.71374585e-02\n",
      "  1.88149344e-02 -1.30892433e-02  1.30065866e-02 -3.13645937e-02\n",
      " -6.34490373e-03 -1.21062268e-02  9.30532999e-03  2.26146858e-02\n",
      " -1.09655214e-02  2.60666981e-02 -1.16989398e-02 -2.86462829e-02\n",
      "  5.46006858e-03  6.94196858e-03 -2.11483594e-02 -7.33413640e-03\n",
      "  2.25870814e-02 -9.00265109e-03 -1.61297980e-03  2.96900999e-02\n",
      "  9.15159471e-04 -9.01567098e-03  1.55106010e-02 -1.18190683e-02\n",
      " -2.17130175e-03  6.21707062e-04 -7.51020154e-03  2.02080477e-02\n",
      " -2.82375049e-02 -3.86153883e-03  1.08342450e-02  1.62482690e-02\n",
      "  5.83569286e-03  4.13950998e-03  5.74169494e-03  2.27569155e-02\n",
      " -3.56603647e-04  1.64645892e-02 -8.48539080e-03  7.38754729e-03\n",
      " -7.76082557e-03  4.16134000e-02 -3.02176690e-03  2.31937179e-03\n",
      " -6.91733323e-03 -2.16634795e-02  2.07443386e-02  4.64670844e-02\n",
      "  6.25056475e-02  3.67431459e-03 -1.81086324e-02  3.88816036e-02\n",
      "  1.01821600e-02  6.67263463e-04  2.67473310e-02  2.27813050e-02\n",
      " -1.92986836e-03 -3.54483724e-02 -5.62232826e-03 -4.60822275e-03\n",
      "  2.38216929e-02  2.07437295e-02 -3.33512612e-02 -7.82656949e-03\n",
      "  8.98422394e-03 -1.46493148e-02  2.22100019e-02  1.26690348e-03\n",
      "  3.14618839e-04 -2.40925029e-02 -3.39046121e-02  1.01313302e-02\n",
      " -1.54434061e-02  5.63592976e-03 -2.12395582e-02 -1.77027506e-03\n",
      " -7.58103654e-03  2.35904921e-02  1.09619461e-02  3.93792875e-02\n",
      "  1.13230497e-02 -2.06324272e-02 -3.85690555e-02  2.55335495e-02\n",
      "  4.89737652e-02 -2.79334513e-03 -5.65007934e-03 -4.77403915e-03\n",
      "  2.65251528e-02  6.53231342e-04 -2.20606080e-03  6.19835733e-03\n",
      "  1.69226509e-02  1.19612757e-02  1.14255073e-02  1.74115412e-02\n",
      "  1.49188247e-02 -1.55720464e-03 -1.10264588e-02  3.92133817e-02\n",
      "  1.12529267e-02 -6.89015491e-03  4.36476199e-03  1.53107718e-02\n",
      " -2.99672000e-02  2.56416891e-02  2.93442281e-03 -2.63166744e-02\n",
      " -1.76880676e-02 -1.58257782e-02 -3.67538678e-03 -5.50363678e-03\n",
      "  2.53883395e-02  1.86869688e-02  1.66557785e-02  3.33565883e-02\n",
      " -2.18580943e-02  2.45290510e-02 -3.38229649e-02 -1.37510020e-02\n",
      "  3.83245424e-05  1.60668902e-02 -1.12003898e-02  1.50411325e-02\n",
      " -5.49684465e-03  2.64677815e-02 -1.95828062e-02 -6.55291975e-03\n",
      " -5.74650429e-03 -4.37913043e-03 -7.24550569e-03 -1.54720619e-02\n",
      "  2.04993319e-02  5.27467579e-04 -4.11170907e-03 -3.88403912e-03\n",
      "  1.73130985e-02  1.50583331e-02 -1.34265737e-03 -3.10433097e-02\n",
      "  3.44411749e-03  1.03958184e-02 -5.93225518e-03  3.31676751e-03\n",
      " -1.12422695e-02 -2.24071033e-02 -3.10117006e-03  2.40380280e-02\n",
      " -1.78193282e-02  2.00728718e-02  8.11298564e-03  3.92688671e-04\n",
      "  1.99369062e-02 -3.76902230e-04  2.08365847e-03  5.49775315e-03\n",
      "  9.38464981e-03 -3.39490846e-02 -7.51261250e-04 -3.34478430e-02\n",
      " -4.35181297e-02  2.25037448e-02 -3.87005247e-02 -1.02875261e-02\n",
      " -4.73404713e-02  6.83119474e-03 -1.38843041e-02  1.82449464e-02\n",
      "  3.98682803e-03  7.61499349e-03  8.10591225e-03 -1.02439141e-02\n",
      " -1.09839793e-02  8.44829623e-03  1.74468886e-02 -1.57774556e-02\n",
      " -2.15308275e-02  2.37627532e-02 -2.84463223e-02  3.66412103e-02\n",
      " -3.42474282e-02  2.51776632e-02  1.46206012e-02  9.92950890e-03\n",
      "  1.87931582e-02  2.43338123e-02  5.12044551e-03 -1.87754240e-02\n",
      " -9.99804121e-03 -3.15019465e-03  2.55939439e-02  1.45850722e-02\n",
      " -1.62803475e-02  2.16979310e-02  2.42292485e-03 -1.16900364e-02\n",
      " -5.51461475e-04  1.05289854e-02  9.32521187e-03 -2.67847497e-02\n",
      " -3.74325067e-02 -1.11001953e-02  1.46736605e-02 -2.34364066e-02\n",
      "  3.28594781e-02 -3.15545201e-02 -2.76714913e-03  1.57641005e-02\n",
      "  7.72379292e-03  3.61478631e-03 -9.08443611e-03 -4.24271263e-02\n",
      "  1.66610628e-02 -2.14654300e-02  1.91425160e-02 -2.27072965e-02\n",
      " -1.85873210e-02 -8.15691147e-03  1.39644751e-02 -3.36002819e-02\n",
      "  1.40684862e-02 -2.17959657e-02  2.69782767e-02  2.94006686e-03\n",
      "  3.09689902e-03  4.27714698e-02  3.57024814e-03 -2.14806460e-02\n",
      " -7.14169024e-03  6.81075966e-03  1.20396065e-02 -1.80015024e-02\n",
      " -1.24845013e-03  9.65526327e-03 -6.04681671e-03 -5.17340656e-03\n",
      "  2.17280425e-02 -1.55170145e-03 -3.69176781e-03  1.84961408e-03\n",
      "  2.47267447e-02  2.27578860e-02  1.75390579e-02 -1.18691511e-02\n",
      " -1.01776505e-02 -1.87920481e-02  3.67744118e-02  1.77009520e-03\n",
      "  2.87359282e-02 -1.00067332e-02 -5.91749267e-04  1.78206395e-02\n",
      "  2.96638012e-02  2.48103309e-03 -3.65675315e-02  2.46491712e-02\n",
      " -8.81561358e-03  1.74642261e-02  2.54799123e-03 -9.65251215e-03\n",
      " -2.05850285e-02  4.88759065e-03 -6.42726757e-03  1.68348718e-02\n",
      " -2.33162995e-02 -8.34431686e-03  2.09382232e-02 -3.91515298e-03\n",
      "  8.07085820e-03  1.72768328e-02  7.13994075e-03 -3.51632424e-02\n",
      " -3.02311871e-02  1.57756708e-03  2.19863374e-03  7.52479071e-03\n",
      "  1.43737877e-02  2.96570025e-02  8.33942182e-03  3.18217166e-02\n",
      " -8.05767998e-03 -8.15945212e-03  5.26399259e-03 -2.58594044e-02\n",
      " -3.49082053e-02  1.89229697e-02  2.02535242e-02  3.44836228e-02\n",
      "  3.02210134e-02  1.65388770e-02 -2.72015613e-02  1.49310264e-03\n",
      "  1.33128148e-02  1.00704078e-02 -7.94187747e-03 -1.96245732e-04\n",
      " -1.94462435e-03  7.71524897e-03 -1.40107814e-02 -1.54615389e-02\n",
      " -2.77151191e-03  8.62749177e-04  1.89009774e-03 -7.03214202e-03\n",
      "  9.10993293e-03 -2.28872299e-02 -4.13136445e-02  1.84798166e-02\n",
      " -1.06859091e-03  3.05429511e-02  2.11443892e-03  1.92984808e-02\n",
      " -5.09312749e-03 -4.79325047e-03  7.04028923e-03  1.57880988e-02\n",
      "  5.37834456e-03 -1.97689291e-02 -1.10099651e-02 -2.89181178e-03\n",
      "  7.52995489e-04  1.05409892e-02 -9.86496569e-04 -3.40925381e-02\n",
      "  1.61252562e-02 -1.13424454e-02  5.83937392e-03  1.67494360e-02\n",
      " -1.02824830e-02 -1.58409178e-02 -8.33606464e-04  1.43914053e-03\n",
      "  1.75602231e-02  9.53780022e-03  1.63503438e-02 -1.45114344e-02\n",
      "  3.93617712e-02  9.40185040e-03  3.88965156e-04 -8.08299426e-03\n",
      " -2.16185432e-02 -1.41237397e-04 -7.44596636e-03 -5.79219265e-03\n",
      "  2.87233442e-02  8.61079991e-03 -3.84678808e-03  1.77714769e-02\n",
      " -7.15868408e-03 -1.15661211e-02 -1.38414633e-02  1.45566501e-02\n",
      " -1.19927516e-02  8.23388156e-03 -1.17259687e-02  3.17885578e-02\n",
      " -3.84318084e-02 -4.12491411e-02 -1.20329792e-02 -3.95028992e-03\n",
      " -1.00556733e-02  4.01198305e-02 -5.11953328e-03  1.88970249e-02\n",
      "  1.49108944e-02  1.06556071e-02  3.34453136e-02  3.08174621e-02\n",
      "  3.31658944e-02 -6.17361721e-03  2.74580084e-02  3.19590792e-02\n",
      " -2.07820199e-02 -1.46404682e-02 -1.47113465e-02 -2.07256265e-02\n",
      " -5.89716323e-02 -4.57060896e-03  8.65004305e-03  1.77958738e-02\n",
      " -1.42301871e-02  2.22846363e-02  2.47655017e-03 -3.15774791e-03\n",
      "  2.82596555e-02 -5.33644250e-03 -3.56351137e-02 -1.81363206e-02\n",
      "  1.65369678e-02 -2.43503414e-02  1.74925029e-02  4.06731945e-03\n",
      " -5.75185381e-03  8.09974130e-03  3.47943343e-02 -3.09430598e-03\n",
      "  3.69385108e-02  2.79127583e-02  8.86379089e-03 -3.24656367e-02\n",
      " -9.25751496e-03  1.69488899e-02 -1.99033543e-02  7.73571944e-03\n",
      "  6.32618740e-03 -2.13569924e-02 -1.35128545e-02  1.50423660e-03\n",
      "  1.78347644e-03 -3.49592138e-03 -4.27065901e-02  3.85227799e-02\n",
      "  1.98529148e-03 -1.11599835e-02 -2.88419612e-02  4.35850695e-02\n",
      " -5.55196265e-03  3.14478832e-03 -4.44879010e-03 -8.75685550e-03\n",
      "  2.23328788e-02  1.93922464e-02  7.93436728e-03  2.70447582e-02\n",
      "  4.18218318e-03  8.66055302e-03  2.82404036e-03 -2.10159868e-02\n",
      "  2.88461316e-02 -2.15362981e-02  8.50016065e-03 -4.65008169e-02\n",
      " -2.00584345e-02 -2.39509828e-02 -2.98047531e-02  1.17964288e-02\n",
      "  1.47235300e-02  1.60657745e-02 -2.64542699e-02  9.17002733e-04\n",
      " -1.88651271e-02 -1.70340873e-02  8.65945686e-03  2.99309678e-02\n",
      " -2.23290101e-02 -2.24625436e-03 -3.57729057e-03 -1.22165903e-02\n",
      " -4.41332487e-03  2.35375846e-04  3.25864647e-03 -1.44858670e-03\n",
      " -7.30746426e-03 -1.88885964e-02 -1.85359288e-02  1.74785834e-02\n",
      " -1.79020874e-02  1.26229748e-02  1.41128199e-02  2.39637587e-03\n",
      " -4.37502842e-03  1.60898492e-02  5.42806238e-02 -8.69392604e-03\n",
      " -3.16074528e-02 -1.22102983e-02 -7.23627722e-03 -2.53267642e-02\n",
      " -2.85613313e-02 -1.20182289e-02 -1.27002131e-02  3.50707360e-02\n",
      " -1.18699565e-03 -1.10083083e-02 -2.03999504e-03  1.30223529e-02\n",
      " -2.84792464e-02  8.34128913e-03 -2.42286175e-02 -1.15458881e-02\n",
      "  1.89117044e-02  5.46254171e-03  3.64299156e-02  9.47371777e-03\n",
      "  4.65802196e-03  1.79581605e-02 -1.79153308e-02  9.98697337e-03\n",
      " -1.84667986e-02  2.20865272e-02 -1.82663242e-03 -4.24982328e-03\n",
      "  6.39837049e-03 -1.27689829e-02 -1.09556839e-02 -2.03343611e-02\n",
      "  1.25274677e-02  5.19751795e-02  1.93227567e-02  4.17355588e-03\n",
      " -5.02258586e-03 -4.23308201e-02 -9.11390875e-03  2.65859757e-02\n",
      "  2.77811214e-02  3.13197486e-02 -1.29936719e-02  4.22045868e-03\n",
      "  1.74281374e-02 -1.55814772e-03  3.20528029e-03  2.09544115e-02\n",
      "  4.20928486e-02 -6.76533440e-03  2.50590034e-02 -2.08293572e-02\n",
      "  2.14726897e-03 -4.46049199e-02  4.35716892e-03 -5.86836459e-03\n",
      "  1.11418804e-02 -1.14962449e-02  2.35665664e-02 -2.29480620e-02\n",
      "  4.67023905e-03  1.59445610e-02  6.69513876e-03 -4.74015959e-02\n",
      " -2.71278750e-02  4.24620276e-03  1.64594445e-02  1.83575414e-02\n",
      "  1.51420692e-02 -5.12960041e-03  1.30384313e-02 -7.86502752e-03\n",
      "  9.64511931e-03  5.71232177e-02 -2.00788826e-02  1.17694796e-03\n",
      " -5.91556029e-03 -1.85132846e-02 -3.65432128e-02 -3.23922783e-02\n",
      "  1.72581021e-02  6.57440163e-03 -8.94267985e-04  9.55647510e-03\n",
      " -5.42725995e-03  6.47911290e-03  2.04220098e-02  9.28078126e-03\n",
      " -1.16954418e-02 -4.10813093e-03 -6.34047901e-03 -2.41675582e-02\n",
      " -1.67649426e-02 -8.09496921e-03 -9.91057046e-03  1.56626385e-02\n",
      "  6.34608697e-03 -3.92094208e-03 -1.45523921e-02  2.24805926e-03\n",
      " -8.31348915e-03  7.56104244e-03 -1.15580400e-02 -5.48796169e-02\n",
      " -1.50747318e-02  2.62301508e-02 -2.60701422e-02 -2.63702255e-02\n",
      "  3.74131240e-02  2.17745025e-02  2.25482639e-02 -2.28642691e-02\n",
      "  3.15642618e-02  1.31033007e-02  8.93358700e-03 -5.86689496e-03\n",
      " -1.24188410e-02 -2.38487069e-02  5.70676476e-03  8.89077783e-03\n",
      "  3.48744984e-03 -1.18516963e-02  1.08719561e-02  1.45939672e-02\n",
      "  9.13853932e-04  1.02737257e-02  6.80988235e-03  8.00120272e-03\n",
      "  1.59658976e-02  3.34128328e-02  2.04570452e-03 -8.88194051e-03\n",
      "  8.49587005e-03  1.80600807e-02 -1.16880229e-02  1.39414603e-02\n",
      " -7.83734582e-03 -2.56322864e-02 -2.99312323e-02  4.04890673e-03\n",
      "  2.23415974e-03 -1.36071574e-02 -2.78300326e-02 -5.70353447e-03\n",
      " -7.81012373e-03  2.73352787e-02 -6.56977855e-03 -1.08254319e-02\n",
      "  2.73929369e-02  1.81428355e-03  2.86188151e-04  1.97132621e-02\n",
      "  2.89989077e-03  1.43886115e-02  3.42326313e-02  5.54808229e-03\n",
      "  2.07007490e-02  2.90517211e-02 -8.93077720e-03 -5.68380952e-03\n",
      "  2.72437036e-02  1.36554763e-02  2.71176714e-02 -2.73821950e-02\n",
      "  1.80480555e-02 -3.92153999e-03  2.07953453e-02 -3.02096829e-02\n",
      " -2.49140039e-02  6.38002157e-03 -4.96822130e-03 -2.59915348e-02\n",
      " -7.07199844e-03  8.76061339e-03  3.32852267e-02  2.27285735e-03\n",
      " -1.47508970e-02 -7.81081757e-03 -5.09427208e-03  7.77549762e-03\n",
      "  2.15964718e-03  7.85831641e-03  9.77315754e-03 -1.41058560e-03\n",
      "  5.11085615e-02  7.56003847e-03  1.26607465e-02 -3.96421775e-02\n",
      "  7.19694700e-03 -1.32669890e-02  2.91572716e-02  5.68729127e-03\n",
      "  3.73639204e-02 -3.49466391e-02  8.86543933e-03 -3.02555021e-02\n",
      " -8.49807914e-03 -2.13355925e-02 -7.10240332e-03  3.49145010e-02\n",
      "  7.32794357e-03  5.16919792e-03  1.61590483e-02 -1.81124154e-02\n",
      "  1.21006621e-02  1.14415577e-02  7.43242865e-03  3.44966189e-03\n",
      "  3.48664299e-02  2.33870577e-02 -5.57457097e-04  1.45241162e-02\n",
      " -4.28612754e-02  7.87347648e-03  3.34410816e-02 -2.58666016e-02\n",
      "  1.36848539e-02 -4.14546719e-03 -2.30435319e-02  3.77291553e-02\n",
      "  1.02356523e-02 -2.13269703e-03 -2.07851510e-02 -1.53521607e-02\n",
      "  2.52195522e-02  7.67708570e-03  2.90048234e-02 -1.66983774e-03\n",
      " -6.34940295e-03 -2.10161228e-02  3.11610587e-02 -1.24694714e-02\n",
      " -5.51629514e-02  2.99183242e-02  5.61826443e-03  1.90158244e-02\n",
      " -2.66564060e-02 -1.60355363e-02  3.05043571e-02  1.01831015e-02\n",
      "  1.50535246e-02 -1.81626640e-02  4.98449604e-04  8.50701798e-03\n",
      " -1.41551970e-02 -2.18911953e-02 -2.23114472e-02  1.09482948e-02\n",
      "  7.87294563e-03  1.40987057e-02 -2.61733565e-03  1.09273307e-02\n",
      " -2.81097624e-03 -2.69677341e-02  6.57612365e-03  8.95897485e-03\n",
      "  2.86877174e-02  7.52745988e-03 -2.00217962e-02 -2.11619027e-02\n",
      " -3.18719670e-02  1.94903929e-02  3.97751890e-02 -1.33084813e-02\n",
      " -1.51925383e-03  2.24149954e-02  1.86827481e-02 -6.01098640e-03]\n"
     ]
    }
   ],
   "source": [
    "# 获取嵌入层\n",
    "embedding_layer = model.roberta.embeddings.word_embeddings\n",
    "\n",
    "# 选择要查看的 PID\n",
    "pid = 12345\n",
    "\n",
    "# 获取 PID 对应的索引\n",
    "idx = pid_to_idx[pid]\n",
    "\n",
    "# 提取嵌入向量\n",
    "embedding = embedding_layer.weight[idx].detach().cpu().numpy()\n",
    "\n",
    "# 打印嵌入向量\n",
    "print(f\"PID: {pid}, \\nEmbedding: {embedding}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
