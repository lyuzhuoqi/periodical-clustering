{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('clustering/')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "VID to index mapping loaded successfully!\n",
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "Citation matrix loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join(get_data_dir(), '2010s', 'direct_citation', 'vid_to_index.json'), 'r') as f:\n",
    "    vid_to_index = json.load(f)\n",
    "print(\"VID to index mapping loaded successfully!\")\n",
    "\n",
    "# 读取稀疏矩阵\n",
    "loaded_data = np.load(os.path.join(get_data_dir(), '2010s', 'direct_citation', 'citation_matrix.npz'))\n",
    "citation_matrix = sp.coo_matrix((loaded_data['data'], (loaded_data['row'], loaded_data['col'])), shape=(len(vid_to_index), len(vid_to_index)))\n",
    "print(\"Citation matrix loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small-World Network Sparse Matrix (COO format):\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 998)\t1\n",
      "  (0, 999)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 999)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 6)\t1\n",
      "  (5, 459)\t1\n",
      "  (6, 4)\t1\n",
      "  :\t:\n",
      "  (993, 994)\t1\n",
      "  (993, 995)\t1\n",
      "  (994, 992)\t1\n",
      "  (994, 993)\t1\n",
      "  (994, 995)\t1\n",
      "  (994, 996)\t1\n",
      "  (995, 993)\t1\n",
      "  (995, 994)\t1\n",
      "  (995, 996)\t1\n",
      "  (995, 997)\t1\n",
      "  (996, 378)\t1\n",
      "  (996, 923)\t1\n",
      "  (996, 994)\t1\n",
      "  (996, 995)\t1\n",
      "  (996, 997)\t1\n",
      "  (997, 80)\t1\n",
      "  (997, 995)\t1\n",
      "  (997, 996)\t1\n",
      "  (997, 998)\t1\n",
      "  (998, 0)\t1\n",
      "  (998, 997)\t1\n",
      "  (998, 999)\t1\n",
      "  (999, 0)\t1\n",
      "  (999, 1)\t1\n",
      "  (999, 998)\t1\n",
      "Sparse Matrix with Edge Weights (COO format):\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t8\n",
      "  (0, 998)\t4\n",
      "  (0, 999)\t8\n",
      "  (1, 0)\t6\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 999)\t7\n",
      "  (2, 0)\t8\n",
      "  (2, 1)\t8\n",
      "  (2, 3)\t10\n",
      "  (2, 4)\t8\n",
      "  (3, 1)\t4\n",
      "  (3, 2)\t9\n",
      "  (3, 4)\t9\n",
      "  (3, 5)\t4\n",
      "  (4, 2)\t5\n",
      "  (4, 3)\t6\n",
      "  (4, 5)\t9\n",
      "  (4, 6)\t4\n",
      "  (5, 3)\t3\n",
      "  (5, 4)\t8\n",
      "  (5, 6)\t9\n",
      "  (5, 459)\t7\n",
      "  (6, 4)\t10\n",
      "  :\t:\n",
      "  (993, 994)\t4\n",
      "  (993, 995)\t3\n",
      "  (994, 992)\t1\n",
      "  (994, 993)\t3\n",
      "  (994, 995)\t3\n",
      "  (994, 996)\t2\n",
      "  (995, 993)\t8\n",
      "  (995, 994)\t2\n",
      "  (995, 996)\t6\n",
      "  (995, 997)\t10\n",
      "  (996, 378)\t5\n",
      "  (996, 923)\t5\n",
      "  (996, 994)\t5\n",
      "  (996, 995)\t3\n",
      "  (996, 997)\t10\n",
      "  (997, 80)\t5\n",
      "  (997, 995)\t9\n",
      "  (997, 996)\t1\n",
      "  (997, 998)\t6\n",
      "  (998, 0)\t2\n",
      "  (998, 997)\t2\n",
      "  (998, 999)\t4\n",
      "  (999, 0)\t3\n",
      "  (999, 1)\t5\n",
      "  (999, 998)\t10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyuzhuoqi/miniconda3/envs/p2v/lib/python3.11/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# 生成小世界网络\n",
    "num_nodes = 1000  # 节点数\n",
    "k = 4  # 每个节点连接的邻居数\n",
    "p = 0.1  # 重连概率\n",
    "G = nx.watts_strogatz_graph(num_nodes, k, p)\n",
    "\n",
    "# 获取邻接矩阵的COO格式\n",
    "citation_matrix = nx.adjacency_matrix(G).tocoo()  # 转换为COO格式\n",
    "\n",
    "# 打印稀疏矩阵的基本信息\n",
    "print(\"Small-World Network Sparse Matrix (COO format):\")\n",
    "print(citation_matrix)\n",
    "\n",
    "# 生成1到10之间的随机权重，赋值给稀疏矩阵的边\n",
    "edge_weights = np.random.randint(1, 11, size=citation_matrix.nnz)\n",
    "citation_matrix.data = edge_weights\n",
    "\n",
    "# 输出带有权重的稀疏矩阵\n",
    "print(\"Sparse Matrix with Edge Weights (COO format):\")\n",
    "print(citation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Index Shape: torch.Size([2, 4000])\n",
      "Edge Attr Shape: torch.Size([4000])\n",
      "Using device: cuda\n",
      "Data x shape: torch.Size([1000, 64])\n",
      "Data edge_index shape: torch.Size([2, 4000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "# 转换稀疏矩阵为 PyTorch Geometric 所需的格式\n",
    "# edge_index 是一个包含边的矩阵（2, E），其中 E 是边的数量\n",
    "# edge_attr 是边的权重\n",
    "edge_index = torch.tensor(np.array(np.nonzero(citation_matrix > 0)), dtype=torch.long)\n",
    "edge_attr = torch.tensor(citation_matrix.data, dtype=torch.float)\n",
    "# 构建 edge_set，存储正样本边集\n",
    "edge_set = set(zip(edge_index[0].cpu().numpy(), edge_index[1].cpu().numpy()))\n",
    "# 确保 edge_index 和 edge_attr 的一致性\n",
    "print(f\"Edge Index Shape: {edge_index.shape}\")\n",
    "print(f\"Edge Attr Shape: {edge_attr.shape}\")\n",
    "\n",
    "# 创建 PyG 的数据对象\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# 假设每个节点的特征是从标准正态分布中随机生成的，维度为 64\n",
    "x = torch.randn((num_nodes, 64))  # 每个节点的特征为一个随机向量（形状为 [n, 64]）\n",
    "\n",
    "# 将节点特征加入 Data 对象\n",
    "data.x = x\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 将数据转移到GPU\n",
    "data = data.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "# 打印数据的基本信息\n",
    "print(f\"Data x shape: {data.x.shape}\")\n",
    "print(f\"Data edge_index shape: {data.edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv  # 使用GATConv\n",
    "import random\n",
    "\n",
    "# 定义 GAT 模型\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        # 使用 GATConv 层来替代 GCNConv\n",
    "        self.gat_conv1 = GATConv(in_channels, out_channels, heads=num_heads)\n",
    "        self.gat_conv2 = GATConv(out_channels * num_heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # 第一层 GAT 卷积\n",
    "        x = self.gat_conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # 第二层 GAT 卷积\n",
    "        x = self.gat_conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 创建 GAT 模型\n",
    "model = GAT(in_channels=64, out_channels=128, num_heads=8).to(device)\n",
    "\n",
    "# 负采样函数：生成负样本\n",
    "def negative_sampling(edge_set, num_nodes, num_samples, device):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_samples:\n",
    "        u = random.randint(0, num_nodes - 1)\n",
    "        v = random.randint(0, num_nodes - 1)\n",
    "        if u != v:\n",
    "            # 检查 u 和 v 是否在正样本的边中\n",
    "            if (u, v) not in edge_set and (v, u) not in edge_set:\n",
    "                neg_samples.append((u, v))\n",
    "\n",
    "    # 将负样本转移到指定设备（GPU）\n",
    "    neg_samples = torch.tensor(neg_samples, dtype=torch.long).t().contiguous().to(device)\n",
    "    return neg_samples\n",
    "\n",
    "# 训练函数\n",
    "def train(data, edge_set, model, optimizer, criterion, epochs=100, num_neg_samples=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 获取模型的节点嵌入\n",
    "        out = model(data)\n",
    "\n",
    "        # 获取正样本的节点对\n",
    "        pos_edge_index = data.edge_index\n",
    "        num_pos_samples = pos_edge_index.size(1)  # 正样本数量\n",
    "        \n",
    "        # 如果没有给定负样本数量，则设置为与正样本数量相同\n",
    "        num_neg_samples = num_neg_samples or num_pos_samples\n",
    "\n",
    "        # 生成负样本\n",
    "        neg_edge_index = negative_sampling(edge_set, len(data.x), num_neg_samples, device)\n",
    "        \n",
    "        # 正样本的嵌入\n",
    "        pos_emb1 = out[pos_edge_index[0]]\n",
    "        pos_emb2 = out[pos_edge_index[1]]\n",
    "        \n",
    "        # 负样本的嵌入\n",
    "        neg_emb1 = out[neg_edge_index[0]]\n",
    "        neg_emb2 = out[neg_edge_index[1]]\n",
    "\n",
    "        # 计算正样本和负样本的内积作为链接预测的相似度\n",
    "        pos_score = (pos_emb1 * pos_emb2).sum(dim=1)\n",
    "        neg_score = (neg_emb1 * neg_emb2).sum(dim=1)\n",
    "\n",
    "        # 标签：1表示正样本，0表示负样本\n",
    "        labels = torch.cat([torch.ones(pos_score.size(0)), torch.zeros(neg_score.size(0))], dim=0).to(device)\n",
    "        scores = torch.cat([pos_score, neg_score], dim=0)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7512\n",
      "Epoch 2/100, Loss: 5145.7310\n",
      "Epoch 3/100, Loss: 62.8023\n",
      "Epoch 4/100, Loss: 291.1995\n",
      "Epoch 5/100, Loss: 360.1705\n",
      "Epoch 6/100, Loss: 331.6060\n",
      "Epoch 7/100, Loss: 178.7528\n",
      "Epoch 8/100, Loss: 39.0891\n",
      "Epoch 9/100, Loss: 51.4761\n",
      "Epoch 10/100, Loss: 62.8760\n",
      "Epoch 11/100, Loss: 52.5718\n",
      "Epoch 12/100, Loss: 36.9284\n",
      "Epoch 13/100, Loss: 28.9054\n",
      "Epoch 14/100, Loss: 26.1920\n",
      "Epoch 15/100, Loss: 20.8785\n",
      "Epoch 16/100, Loss: 14.5546\n",
      "Epoch 17/100, Loss: 11.2020\n",
      "Epoch 18/100, Loss: 11.3542\n",
      "Epoch 19/100, Loss: 10.2788\n",
      "Epoch 20/100, Loss: 10.1894\n",
      "Epoch 21/100, Loss: 7.9771\n",
      "Epoch 22/100, Loss: 6.5460\n",
      "Epoch 23/100, Loss: 5.8507\n",
      "Epoch 24/100, Loss: 5.2182\n",
      "Epoch 25/100, Loss: 4.3573\n",
      "Epoch 26/100, Loss: 4.1593\n",
      "Epoch 27/100, Loss: 3.5584\n",
      "Epoch 28/100, Loss: 3.0489\n",
      "Epoch 29/100, Loss: 3.1759\n",
      "Epoch 30/100, Loss: 2.5134\n",
      "Epoch 31/100, Loss: 2.6336\n",
      "Epoch 32/100, Loss: 2.2371\n",
      "Epoch 33/100, Loss: 2.0251\n",
      "Epoch 34/100, Loss: 2.0785\n",
      "Epoch 35/100, Loss: 1.7828\n",
      "Epoch 36/100, Loss: 1.7306\n",
      "Epoch 37/100, Loss: 1.5284\n",
      "Epoch 38/100, Loss: 1.5220\n",
      "Epoch 39/100, Loss: 1.3981\n",
      "Epoch 40/100, Loss: 1.3394\n",
      "Epoch 41/100, Loss: 1.1614\n",
      "Epoch 42/100, Loss: 1.1291\n",
      "Epoch 43/100, Loss: 0.9950\n",
      "Epoch 44/100, Loss: 0.9658\n",
      "Epoch 45/100, Loss: 0.9999\n",
      "Epoch 46/100, Loss: 0.9556\n",
      "Epoch 47/100, Loss: 0.8861\n",
      "Epoch 48/100, Loss: 0.8623\n",
      "Epoch 49/100, Loss: 0.8455\n",
      "Epoch 50/100, Loss: 0.8348\n",
      "Epoch 51/100, Loss: 0.7425\n",
      "Epoch 52/100, Loss: 0.7172\n",
      "Epoch 53/100, Loss: 0.7717\n",
      "Epoch 54/100, Loss: 0.7070\n",
      "Epoch 55/100, Loss: 0.6852\n",
      "Epoch 56/100, Loss: 0.6929\n",
      "Epoch 57/100, Loss: 0.6732\n",
      "Epoch 58/100, Loss: 0.6577\n",
      "Epoch 59/100, Loss: 0.6018\n",
      "Epoch 60/100, Loss: 0.6025\n",
      "Epoch 61/100, Loss: 0.5873\n",
      "Epoch 62/100, Loss: 0.5611\n",
      "Epoch 63/100, Loss: 0.5783\n",
      "Epoch 64/100, Loss: 0.5527\n",
      "Epoch 65/100, Loss: 0.5532\n",
      "Epoch 66/100, Loss: 0.5465\n",
      "Epoch 67/100, Loss: 0.5898\n",
      "Epoch 68/100, Loss: 0.5531\n",
      "Epoch 69/100, Loss: 0.5554\n",
      "Epoch 70/100, Loss: 0.5140\n",
      "Epoch 71/100, Loss: 0.5359\n",
      "Epoch 72/100, Loss: 0.5473\n",
      "Epoch 73/100, Loss: 0.5425\n",
      "Epoch 74/100, Loss: 0.5326\n",
      "Epoch 75/100, Loss: 0.5206\n",
      "Epoch 76/100, Loss: 0.5474\n",
      "Epoch 77/100, Loss: 0.5174\n",
      "Epoch 78/100, Loss: 0.5117\n",
      "Epoch 79/100, Loss: 0.5062\n",
      "Epoch 80/100, Loss: 0.4918\n",
      "Epoch 81/100, Loss: 0.4870\n",
      "Epoch 82/100, Loss: 0.4807\n",
      "Epoch 83/100, Loss: 0.4988\n",
      "Epoch 84/100, Loss: 0.4998\n",
      "Epoch 85/100, Loss: 0.4802\n",
      "Epoch 86/100, Loss: 0.5071\n",
      "Epoch 87/100, Loss: 0.4911\n",
      "Epoch 88/100, Loss: 0.4957\n",
      "Epoch 89/100, Loss: 0.4815\n",
      "Epoch 90/100, Loss: 0.4826\n",
      "Epoch 91/100, Loss: 0.4764\n",
      "Epoch 92/100, Loss: 0.4895\n",
      "Epoch 93/100, Loss: 0.4727\n",
      "Epoch 94/100, Loss: 0.4780\n",
      "Epoch 95/100, Loss: 0.4777\n",
      "Epoch 96/100, Loss: 0.4651\n",
      "Epoch 97/100, Loss: 0.4874\n",
      "Epoch 98/100, Loss: 0.4693\n",
      "Epoch 99/100, Loss: 0.4647\n",
      "Epoch 100/100, Loss: 0.4878\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数和优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 开始训练\n",
    "train(data, edge_set, model, optimizer, criterion, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
